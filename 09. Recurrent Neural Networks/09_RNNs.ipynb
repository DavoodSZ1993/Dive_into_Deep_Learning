{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGS9wlzL+h9Q4dEOfmjHTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavoodSZ1993/Dive_into_Deep_Learning/blob/main/09_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNY5J2rAE6Uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c82d8d1-a249-4f71-c6ff-3f42a6f06655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0 --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1 Working with Sequences"
      ],
      "metadata": {
        "id": "xkjFgTsDFTUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.3 Training"
      ],
      "metadata": {
        "id": "FluZl0H8FZ-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "ApfjfO6XFc_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(d2l.DataModule):\n",
        "  def __init__(self, batch_size=16, T=1000, num_train=600, tau=4):\n",
        "    self.save_hyperparameters()\n",
        "    self.time = torch.arange(1, T + 1, dtype=torch.float32)      # T = 1, 0000, 1000\n",
        "    self.x = torch.sin(0.01 * self.time) + torch.randn(T) * 0.2  # x = sin(0.01T) + N(0, 1) * 0.2"
      ],
      "metadata": {
        "id": "rdEIcm4xGBx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Data()\n",
        "d2l.plot(data.time, data.x, 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
      ],
      "metadata": {
        "id": "tWKufMZeHG0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(Data)\n",
        "def get_dataloader(self, train):\n",
        "  features = [self.x[i : self.T - self.tau + i] for i in range(self.tau)]\n",
        "  self.features = torch.stack(features, 1)\n",
        "  self.labels = self.x[self.tau:].reshape((-1, 1))\n",
        "  i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
        "  return self.get_tensorloader([self.features, self.labels], train, i)\n"
      ],
      "metadata": {
        "id": "GwegzyIDHibr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = d2l.LinearRegression(lr=0.01)\n",
        "data = Data()\n",
        "trainer = d2l.Trainer(max_epochs=5)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "g5gr5IvQPnoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.4 Prediction"
      ],
      "metadata": {
        "id": "QUe9P4NmP-sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onestep_preds = model(data.features).detach().numpy()\n",
        "d2l.plot(data.time[data.tau:], [data.labels, onestep_preds], 'time', 'x',\n",
        "         legend=['labels', '1-step preds'], figsize=(6, 3))"
      ],
      "metadata": {
        "id": "fksNM0HVQCRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multistep_preds = torch.zeros(data.T)\n",
        "multistep_preds [:] = data.x\n",
        "\n",
        "for i in range(data.num_train + data.tau, data.T):\n",
        "  multistep_preds[i] = model(multistep_preds[i - data.tau:i].reshape((1, -1)))\n",
        "\n",
        "multistep_preds = multistep_preds.detach().numpy()"
      ],
      "metadata": {
        "id": "RoQvbccKQ8Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.plot([data.time[data.tau:], data.time[data.num_train + data.tau:]],\n",
        "         [onestep_preds, multistep_preds[data.num_train + data.tau:]],\n",
        "         'time', 'x', legend=['1-step preds', 'multi-step preds'], figsize=(6, 3))"
      ],
      "metadata": {
        "id": "kWfX6F7ZRqiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_step_pred(k):\n",
        "  features = []\n",
        "  for i in range(data.tau):\n",
        "    features.append(data.x[i: i+data.T - data.tau-k+1])\n",
        "\n",
        "  for i in range(k):\n",
        "    preds = model(torch.stack(features[i : i+data.tau], 1))\n",
        "    features.append(preds.reshape(-1))\n",
        "\n",
        "  return features[data.tau:]"
      ],
      "metadata": {
        "id": "BeS-aCQDTNWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = (1, 4, 16, 64)\n",
        "preds = k_step_pred(steps[-1])\n",
        "\n",
        "d2l.plot(data.time[data.tau+steps[-1]-1:],\n",
        "         [preds[k-1].detach().numpy() for k in steps], 'time', 'x',\n",
        "         legend=[f'{k}-step preds' for k in steps], figsize=(6, 3))"
      ],
      "metadata": {
        "id": "9Y0xa29oT-85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2 Converting Raw Text into Sequence Data"
      ],
      "metadata": {
        "id": "A3wPT3F766WJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "B24IubFY7AGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2.1 Reading the Dataset"
      ],
      "metadata": {
        "id": "0-aMPF1Y7NN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeMachine(d2l.DataModule):\n",
        "  def _download(self):\n",
        "    fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,\n",
        "                         '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
        "    with open(fname) as f:\n",
        "      return f.read()"
      ],
      "metadata": {
        "id": "uPiVKYc57T20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = TimeMachine()\n",
        "raw_text = data._download()\n",
        "raw_text[:60]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zWpTEO3371Wn",
        "outputId": "1f2e6986-5ee5-4e6e-881b-c2a43c0a7cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Time Machine, by H. G. Wells [1898]\\n\\n\\n\\n\\nI\\n\\n\\nThe Time Tra'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(TimeMachine)\n",
        "def _preprocess(self, text):\n",
        "  return re.sub('[^A-Za-z]+', ' ', text).lower()"
      ],
      "metadata": {
        "id": "92tUpDyH8s0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = data._preprocess(raw_text)\n",
        "text[0:60]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mY66zrW-9DC2",
        "outputId": "28c9fe49-03f2-4224-b71e-764bd185307b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the time machine by h g wells i the time traveller for so it'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2.2 Tokenization"
      ],
      "metadata": {
        "id": "5Vjcr_W89efc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(TimeMachine)\n",
        "def _tokenize(self, text):\n",
        "  return list(text)"
      ],
      "metadata": {
        "id": "GmhoOXcC9hkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = data._tokenize(text)\n",
        "','.join(tokens[:30]), tokens[0]   # A 1D list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOg7jhJ491Y-",
        "outputId": "af1f3fbd-2bc9-43d2-8b74-58e6af1dcfa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('t,h,e, ,t,i,m,e, ,m,a,c,h,i,n,e, ,b,y, ,h, ,g, ,w,e,l,l,s, ', 't')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2.3 Vocabulary"
      ],
      "metadata": {
        "id": "iELOxXQw-iRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "  def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "    # Flatten a 2D list if needed.\n",
        "    if tokens and isinstance(tokens[0], list):\n",
        "      tokens = [token for line in tokens for token in line]    # nested-list comprehension\n",
        "    \n",
        "    # Count token frequencies\n",
        "    counter = collections.Counter(tokens)\n",
        "    self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                              reverse=True)\n",
        "    \n",
        "    # The list of unique tokens\n",
        "    self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
        "        token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "    self.token_to_idx = {token: idx\n",
        "                         for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk)\n",
        "    return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "  def to_tokens(self, indices):\n",
        "    if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "      return [self.idx_to_token[int(index)] for index in indices]\n",
        "    return self.idx_to_token[indices]\n",
        "\n",
        "  @property\n",
        "  def unk(self):\n",
        "    return self.token_to_idx['<unk>']"
      ],
      "metadata": {
        "id": "nlj6zj67-lKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Vocab(tokens)\n",
        "indices = vocab[tokens[:10]] # __getitem__() magic method \n",
        "print('indices: ', indices)\n",
        "print('words: ', vocab.to_tokens(indices))"
      ],
      "metadata": {
        "id": "oH2J2n9yG-wb",
        "outputId": "40ade899-a409-481f-d67c-67698dba0c05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indices:  [21, 9, 6, 0, 21, 10, 14, 6, 0, 14]\n",
            "words:  ['t', 'h', 'e', ' ', 't', 'i', 'm', 'e', ' ', 'm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2.4 Putting It All Together"
      ],
      "metadata": {
        "id": "eiO_QXgzNrge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(TimeMachine)\n",
        "def build(self, raw_text, vocab=None):\n",
        "  tokens = self._tokenize(self._preprocess(raw_text))\n",
        "  if vocab is None: vocab = Vocab(tokens)\n",
        "  corpus = [vocab[token] for token in tokens]\n",
        "  return corpus, vocab"
      ],
      "metadata": {
        "id": "0E6GNMBlOEUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus, vocab = data.build(raw_text)\n",
        "len(corpus), len(vocab) # __len__() magic method"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh9NXgreOuxE",
        "outputId": "f3ae8f04-6a0b-4d6b-f7f1-2cf0483da1f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(173428, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2.5 Exploratory Language Statistics"
      ],
      "metadata": {
        "id": "PtPUY97bQkmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = text.split()\n",
        "vocab = Vocab(words)\n",
        "vocab.token_freqs[:10]"
      ],
      "metadata": {
        "id": "O_3c5GJuQrKu",
        "outputId": "54b40a23-8d37-4b06-fdfa-cfb743ea5200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 2261),\n",
              " ('i', 1267),\n",
              " ('and', 1245),\n",
              " ('of', 1155),\n",
              " ('a', 816),\n",
              " ('to', 695),\n",
              " ('was', 552),\n",
              " ('in', 541),\n",
              " ('that', 443),\n",
              " ('my', 440)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = [freq for token, freq in vocab.token_freqs]\n",
        "d2l.plot(freqs, xlabel='token: x', ylabel='frequncy: n(x)',\n",
        "         xscale='log', yscale='log')"
      ],
      "metadata": {
        "id": "JH2JUC3gQ5Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]\n",
        "bigram_vocab = Vocab(bigram_tokens)\n",
        "bigram_vocab.token_freqs[:10]"
      ],
      "metadata": {
        "id": "4eWxi44iRWwe",
        "outputId": "0883cd3e-648c-423f-ac8f-98678341ebcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('of--the', 309),\n",
              " ('in--the', 169),\n",
              " ('i--had', 130),\n",
              " ('i--was', 112),\n",
              " ('and--the', 109),\n",
              " ('the--time', 102),\n",
              " ('it--was', 99),\n",
              " ('to--the', 85),\n",
              " ('as--i', 78),\n",
              " ('of--a', 73)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_tokens = ['--'.join(triple) for triple in zip(\n",
        "    words[:-2], words[1:-1], words[2:])]\n",
        "trigram_vocab = Vocab(trigram_tokens)\n",
        "trigram_vocab.token_freqs[:10]"
      ],
      "metadata": {
        "id": "nCfnoYl9Rztj",
        "outputId": "5e43b264-3725-4e58-eefd-c5d4f110e06d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the--time--traveller', 59),\n",
              " ('the--time--machine', 30),\n",
              " ('the--medical--man', 24),\n",
              " ('it--seemed--to', 16),\n",
              " ('it--was--a', 15),\n",
              " ('here--and--there', 15),\n",
              " ('seemed--to--me', 14),\n",
              " ('i--did--not', 14),\n",
              " ('i--saw--the', 13),\n",
              " ('i--began--to', 13)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\n",
        "trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\n",
        "\n",
        "d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n",
        "         ylabel='frequency: n(x)', xscale='log', yscale='log',\n",
        "         legend=['unigram', 'bigram', 'trigram'])"
      ],
      "metadata": {
        "id": "kcbNSyNbSTF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3 Language Models"
      ],
      "metadata": {
        "id": "NHz6hF1zmY8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3.3 Partitioning Sequences"
      ],
      "metadata": {
        "id": "Mu0eVFi-mjA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "MJZK8IJYmt4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(d2l.TimeMachine)\n",
        "def __init__(self, batch_size, num_steps, num_train=10000, num_val=5000):\n",
        "  super(d2l.TimeMachine, self).__init__()\n",
        "  self.save_hyperparameters()\n",
        "\n",
        "  corpus, self.vocab = self.build(self._download())\n",
        "  array = torch.tensor([corpus[i:i+num_steps+1]\n",
        "                        for i in range(len(corpus)-num_steps)])\n",
        "  self.X, self.Y = array[:, :-1], array[:, 1:]"
      ],
      "metadata": {
        "id": "D5U_w0b6m_OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(d2l.TimeMachine)\n",
        "def get_dataloader(self, train):\n",
        "  idx = slice(0, self.num_train) if train else slice(\n",
        "      self.num_train, self.num_train + self.num_val)\n",
        "  return self.get_tensorloader([self.X, self.Y], train, idx)"
      ],
      "metadata": {
        "id": "u-j69w1FoCZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = d2l.TimeMachine(batch_size=2, num_steps=10)\n",
        "for X, Y in data.train_dataloader():\n",
        "  print('X: ', X,'\\nY: ', Y)\n",
        "  break"
      ],
      "metadata": {
        "id": "J133zDzvoiOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4 Recurrent Neural Network"
      ],
      "metadata": {
        "id": "7F58ESE8Os9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.4.2 Recurrent Neural Networks with Hidden State"
      ],
      "metadata": {
        "id": "eLWIJ2y3Owmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "WaOlbtJrO2_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, W_xh = torch.randn(3, 1), torch.randn(1, 4)\n",
        "H, W_hh = torch.randn(3, 4), torch.randn(4, 4)\n",
        "torch.matmul(X, W_xh) + torch.matmul(H, W_hh)  # XW_xh + HW_hh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDduDauHO_uW",
        "outputId": "8fd4efbb-8ae3-46c7-b7e1-4b6bdf7eae44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.4947, -2.2974,  0.3234, -1.8619],\n",
              "        [-3.5675, -0.9275,  0.3090,  0.7030],\n",
              "        [ 4.4709, -2.4468,  1.0109, -2.5766]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk9zS8_BPjWJ",
        "outputId": "43557df5-af1c-4329-ef11-41dd49e6158a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.4947, -2.2974,  0.3234, -1.8619],\n",
              "        [-3.5675, -0.9275,  0.3090,  0.7030],\n",
              "        [ 4.4709, -2.4468,  1.0109, -2.5766]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5 Recurrent Neural Network Implementation from Scratch"
      ],
      "metadata": {
        "id": "BJmFURLOZ2h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "fcrU-_cXZ91e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.5.1 RNN Model"
      ],
      "metadata": {
        "id": "LFAOE8fcaS0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNScratch(d2l.Module):\n",
        "  def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
        "    super().__init__()\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    self.W_xh = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n",
        "    self.W_hh = nn.Parameter(torch.randn(num_hiddens, num_hiddens) * sigma)\n",
        "    self.b_h = nn.Parameter(torch.zeros(num_hiddens))"
      ],
      "metadata": {
        "id": "Z9CIBVhNack0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(RNNScratch)\n",
        "def forward(self, inputs, state=None):\n",
        "  if state is None:\n",
        "    state = torch.zeros((inputs.shape[1], self.num_hiddens),\n",
        "                        device=inputs.device)\n",
        "  else:\n",
        "    state, = state\n",
        "  outputs = []\n",
        "  for X in inputs:\n",
        "    state = torch.tanh(torch.matmul(X, self.W_xh) + \n",
        "                       torch.matmul(state, self.W_hh) + self.b_h)\n",
        "    outputs.append(state)\n",
        "  return outputs, state"
      ],
      "metadata": {
        "id": "rNtJoLTNfcX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100\n",
        "rnn = RNNScratch(num_inputs, num_hiddens)\n",
        "\n",
        "X = torch.ones((num_steps, batch_size, num_inputs))\n",
        "outputs, state = rnn(X)"
      ],
      "metadata": {
        "id": "Hkr69rqSgr4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_len(a, n):\n",
        "  assert len(a) == n, f'list\\'s len {len(a)} != expected length {n}'\n",
        "\n",
        "def check_shape(a, shape):\n",
        "  assert a.shape == shape, f'tensor\\'s shape {a.shape} != expected shape {shape}'"
      ],
      "metadata": {
        "id": "93cxUtzphTp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_len(outputs, num_steps)                        # number of outputs is equal to number of time steps\n",
        "check_shape(outputs[0], (batch_size, num_hiddens))  # Initial State\n",
        "check_shape(state, (batch_size, num_hiddens))       # Intermediate states"
      ],
      "metadata": {
        "id": "gKhXhu0uh9hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.5.2 RNN-based Language Model"
      ],
      "metadata": {
        "id": "71BFm5c6yHkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLMScratch(d2l.Classifier):\n",
        "  def __init__(self, rnn, vocab_size, lr=0.01):\n",
        "    super().__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.init_params()\n",
        "\n",
        "  def init_params(self):\n",
        "    self.W_hq = nn.Parameter(torch.randn(                               # Initializes outputs parameters (O_t = H_tW_hq + b_q)\n",
        "        self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)\n",
        "    self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n",
        "\n",
        "  def training_step(self, batch):\n",
        "    l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "    self.plot('ppl', torch.exp(l), train=True)\n",
        "    return l\n",
        "\n",
        "  def validation_step(self, batch):\n",
        "    l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "    self.plot('ppl', torch.exp(l), train=False)"
      ],
      "metadata": {
        "id": "KttdHb-IyM-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One-Hot Encoding"
      ],
      "metadata": {
        "id": "Cq7-NuzCzqoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "F.one_hot(torch.tensor([0, 2]), 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCXXMfftzuxS",
        "outputId": "44be25f0-4ea0-43fa-b2c9-76d202b4e954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(RNNLMScratch)\n",
        "def one_hot(self, X):\n",
        "  return F.one_hot(X.T, self.vocab_size).type(torch.float32)   # the output elements are integer and should be converted to float"
      ],
      "metadata": {
        "id": "eBTyzWwaz3Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transforming RNN Outputs"
      ],
      "metadata": {
        "id": "yHP-8Exu0Jh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(RNNLMScratch)\n",
        "def output_layer(self, rnn_outputs):\n",
        "  outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]        # O_t = H_tW_hq + b_q\n",
        "  return torch.stack(outputs, 1)                                                # outputs should be the same size\n",
        "\n",
        "@d2l.add_to_class(RNNLMScratch)\n",
        "def forward(self, X, state=None):\n",
        "  embs = self.one_hot(X)\n",
        "  rnn_outputs, _ = self.rnn(embs, state)\n",
        "  return self.output_layer(rnn_outputs)"
      ],
      "metadata": {
        "id": "aKgGK3wM0Ni4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNLMScratch(rnn, num_inputs)                                           # num_inputs = vocab_size\n",
        "outputs = model(torch.ones((batch_size, num_steps), dtype=torch.int64))         # one_hot only gets integer inputs and float numbers should be coverted to int\n",
        "check_shape(outputs, (batch_size, num_steps, num_inputs))"
      ],
      "metadata": {
        "id": "OrJGD3-71Ga5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.5.3 Gradient Clipping"
      ],
      "metadata": {
        "id": "eUJfmv2ifaZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(d2l.Trainer)\n",
        "def clip_gradients(self, grad_clip_val, model):\n",
        "  params = [p for p in model.parameters() if p.requires_grad]\n",
        "  norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
        "  if norm > grad_clip_val:\n",
        "    for param in params:\n",
        "      param.grad[:] *= grad_clip_val / norm "
      ],
      "metadata": {
        "id": "XyoqUaHGfeB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.5.4 Training"
      ],
      "metadata": {
        "id": "NmOjvqC3hX6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
        "rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
        "model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)\n",
        "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "rUHpudFMhad4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.5.5 Decoding"
      ],
      "metadata": {
        "id": "WouOu_7AmBtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(RNNLMScratch)\n",
        "def predict(self, prefix, num_preds, vocab, device=None):\n",
        "  state, outputs = None, [vocab[prefix[0]]]\n",
        "  for i in range(len(prefix) + num_preds -1):              # Indexing starts from zero\n",
        "    X = torch.tensor([[outputs[-1]]], device=device)\n",
        "    embs = self.one_hot(X)\n",
        "    rnn_outputs, state = self.rnn(embs, state)\n",
        "    if i < len(prefix) -1: # warm-up period\n",
        "      outputs.append(vocab[prefix[i+1]])\n",
        "    else:\n",
        "      Y = self.output_layer(rnn_outputs)\n",
        "      outputs.append(int(Y.argmax(axis=2).reshape(1)))\n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs]) "
      ],
      "metadata": {
        "id": "nXwNX90UmGMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qiNxs0FDnnwd",
        "outputId": "1de27820-a7f2-4f5e-8eaa-5527909448dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it has of the mere the the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.6 Concise Implementation of Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "KFxGhFVjwPOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "cdPSYIKrwY-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.6.1 Defining the Model"
      ],
      "metadata": {
        "id": "MUpNpi5ywlor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(d2l.Module):\n",
        "  def __init__(self, num_inputs, num_hiddens):\n",
        "    super().__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.rnn = nn.RNN(num_inputs, num_hiddens)\n",
        "\n",
        "  def forward(self, inputs, H=None):   # H: Initial hidden state\n",
        "    return self.rnn(inputs, H)"
      ],
      "metadata": {
        "id": "b0ML4uinwouV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNLM(d2l.RNNLMScratch):\n",
        "  def init_params(self):\n",
        "    self.linear = nn.LazyLinear(self.vocab_size)\n",
        "\n",
        "  def output_layer(self, hiddens):\n",
        "    return self.linear(hiddens).swapaxes(0, 1)        # alias to torch.transpose()"
      ],
      "metadata": {
        "id": "TtYnl02rxQyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.6.2 Training and Predicting"
      ],
      "metadata": {
        "id": "4UMj9El6xtjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
        "rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)\n",
        "model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)\n",
        "model.predict('it has ', 20, data.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "5tas4ms6xy2x",
        "outputId": "f5a6e028-41c6-4b3b-906f-66c670283a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'it has mmmacmmlmmmacmmlmmma'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "c2Jrg55Iyamk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
